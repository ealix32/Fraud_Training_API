# -*- coding: utf-8 -*-
"""Fraud_API.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C8p8qRVt4OSpGrMFk299Cq26Ve8boSjs

# PREPROCESAMIENTO / CALIDAD


---
"""

import os
import re
import unicodedata
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report,
    roc_auc_score,
    roc_curve,
    confusion_matrix,
    precision_score,
    recall_score,
    f1_score,
    accuracy_score,
)

CSV_PATH = "fraud_train.csv"
TARGET_RAW = "FraudFound_P"
TARGET = "is_fraud"
USE_KNN_NUM = False
KNN_K = 5
RARE_THRESHOLD = 0.01

def _strip_accents(x):
    if not isinstance(x, str):
        return x
    return "".join(ch for ch in unicodedata.normalize("NFKD", x) if not unicodedata.combining(ch))

def _norm_text(x):
    if not isinstance(x, str):
        return x
    x = _strip_accents(x).lower().strip()
    return re.sub(r"\s+", " ", x)

def infer_types(df, target=None):
    num = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c != target]
    cat = [c for c in df.columns if (df[c].dtype == "O" or pd.api.types.is_string_dtype(df[c])) and c != target]
    for c in df.columns:
        if c not in cat and c not in num and c != target:
            if pd.api.types.is_numeric_dtype(df[c]) and df[c].nunique(dropna=True) <= 20:
                cat.append(c)
    num = [c for c in num if c not in cat and c != target]
    return cat, num

def standardize_cats(df, cat_cols, rare_thr=0.01):
    df = df.copy()
    for c in cat_cols:
        if c in df.columns:
            s = df[c].astype("object")
            s = s.map(lambda v: _norm_text(v) if pd.notna(v) else v)
            s = s.replace({"": np.nan, "na": np.nan, "n/a": np.nan, "none": np.nan, "null": np.nan})
            freq = s.value_counts(normalize=True, dropna=True)
            rare = set(freq[freq < rare_thr].index)
            s = s.map(lambda v: ("other" if v in rare else v) if pd.notna(v) else v)
            df[c] = s
    return df

def impute(df, cat_cols, num_cols, use_knn=False, k=5):
    df = df.copy()
    if cat_cols:
        df[cat_cols] = SimpleImputer(strategy="most_frequent").fit_transform(df[cat_cols])
    if num_cols:
        if use_knn:
            df[num_cols] = KNNImputer(n_neighbors=k, weights="distance").fit_transform(df[num_cols])
        else:
            df[num_cols] = SimpleImputer(strategy="median").fit_transform(df[num_cols])
    return df

def ensure_binary_target(df, target_final):
    y = df[target_final]
    if y.dropna().isin([0, 1, True, False]).all():
        df[target_final] = y.astype(int)
        return df
    m = {"yes": 1, "y": 1, "si": 1, "sí": 1, "true": 1, "fraud": 1, "no": 0, "n": 0, "false": 0, "legit": 0}
    yy = y.astype(str).str.lower().map(m)
    yy = yy.fillna(pd.to_numeric(y, errors="coerce"))
    yy = yy.fillna(0).astype(float)
    df[target_final] = (yy > 0.5).astype(int)
    return df

def preprocess(csv_in, target_raw, target_final):
    df = pd.read_csv(csv_in)
    if target_raw in df.columns and target_final not in df.columns:
        df = df.rename(columns={target_raw: target_final})
    if target_final not in df.columns:
        raise KeyError(f"Columna objetivo ausente: {target_final}")
    df = df.drop_duplicates(keep="first")
    if "ID" in df.columns:
        df = df.drop(columns=["ID"])
    cats, nums = infer_types(df, target=target_final)
    df = standardize_cats(df, cats, rare_thr=RARE_THRESHOLD)
    df = impute(df, cats, nums, use_knn=USE_KNN_NUM, k=KNN_K)
    df = ensure_binary_target(df, target_final)
    return df

df = preprocess(CSV_PATH, TARGET_RAW, TARGET)
cats, nums = infer_types(df, target=TARGET)

"""# EDA ESENCIAL


---


"""

import textwrap

TOP_N_BARS = 25
FIGSIZE_CAT = (12, 6)
FIGSIZE_CORR = (10, 8)

def _wrap_ylabels(ax, width):
    labels = []
    for lab in ax.get_yticklabels():
        labels.append(textwrap.fill(lab.get_text(), width=width))
    plt.yticks(ax.get_yticks(), labels) # Use plt.yticks instead of ax.set_yticklabels

def univariate_categorical(df, cat_cols, topn=TOP_N_BARS, figsize=FIGSIZE_CAT):
    results = {}
    for col in cat_cols:
        vc = df[col].value_counts(dropna=False)
        freq = (vc / len(df)).rename("pct")
        summary = pd.concat([vc.rename("count"), freq], axis=1).reset_index(names=col)
        results[col] = summary
        plt.figure(figsize=figsize)
        ax = sns.barplot(data=summary.head(topn), x="pct", y=col, color="#4472c4")
        plt.title(f"Distribución de {col} (Top {topn})")
        plt.xlabel("Proporción")
        plt.ylabel(col)
        _wrap_ylabels(ax, 20)
        plt.tight_layout()
        plt.show()
    return results

def fraud_rate_by_category(df, cat_cols, target, topn=TOP_N_BARS, figsize=FIGSIZE_CAT):
    rates = {}
    for col in cat_cols:
        tmp = (
            df.groupby(col, dropna=False)[target]
              .agg(["mean", "count"])
              .rename(columns={"mean": "fraud_rate", "count": "n"})
              .sort_values("fraud_rate", ascending=False)
        )
        rates[col] = tmp.reset_index()
        plt.figure(figsize=figsize)
        ax = sns.barplot(data=tmp.reset_index().head(topn), x="fraud_rate", y=col, palette="flare", hue=col, legend=False)
        plt.title(f"Tasa de fraude por {col} (Top {topn})")
        plt.xlabel("Tasa de fraude")
        plt.ylabel(col)
        plt.xlim(0, 1)
        _wrap_ylabels(ax, 20)
        plt.tight_layout()
        plt.show()
    return rates

def correlation_heatmap(df, num_cols, target, figsize=FIGSIZE_CORR):
    cols = [c for c in num_cols if df[c].notna().any()]
    corr = df[[target] + cols].corr(numeric_only=True)
    plt.figure(figsize=figsize)
    sns.heatmap(corr, annot=False, cmap="coolwarm", center=0)
    plt.title("Matriz de correlación (incluye target)")
    plt.tight_layout()
    plt.show()
    return corr

def discriminative_variables(df, cat_cols, num_cols, target, top_k=15):
    cat_rows = []
    for col in cat_cols:
        r = df.groupby(col, dropna=False)[target].mean()
        if len(r) >= 2:
            cat_rows.append({
                "variable": col,
                "spread_fraud_rate": float(r.max() - r.min()),
                "max_rate": float(r.max()),
                "min_rate": float(r.min()),
                "n_cats": int(r.shape[0])
            })
    cat_rank = pd.DataFrame(cat_rows).sort_values("spread_fraud_rate", ascending=False).head(top_k)
    num_rows = []
    for col in num_cols:
        if df[col].notna().sum() > 2:
            c = df[[col, target]].corr(numeric_only=True).iloc[0, 1]
            num_rows.append({
                "variable": col,
                "abs_corr_with_target": abs(float(c)) if pd.notna(c) else np.nan,
                "corr": float(c) if pd.notna(c) else np.nan
            })
    num_rank = (
        pd.DataFrame(num_rows)
          .dropna(subset=["abs_corr_with_target"])
          .sort_values("abs_corr_with_target", ascending=False)
          .head(top_k)
    )
    return cat_rank, num_rank

uni = univariate_categorical(df, cats) if len(cats) else {}
rates = fraud_rate_by_category(df, cats, TARGET) if len(cats) else {}
corr = correlation_heatmap(df, nums, TARGET) if len(nums) else pd.DataFrame()
cat_rank, num_rank = discriminative_variables(df, cats, nums, TARGET, top_k=15)
print(cat_rank.to_string(index=False))
print(num_rank.to_string(index=False))

"""# MODELO Y EVALUACIÓN


---




"""

print("=== MODELO Y EVALUACIÓN ===")

def build_pipeline(cat_cols, num_cols):
    ct = ColumnTransformer(
        transformers=[
            ("num", StandardScaler(), num_cols),
            ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),
        ]
    )
    pipe = Pipeline(steps=[("prep", ct), ("clf", LogisticRegression(max_iter=1000, solver="liblinear", class_weight="balanced"))])
    return pipe

def ks_stat(y_true, y_sc):
    d = pd.DataFrame({"y": y_true, "s": y_sc}).sort_values("s", ascending=False)
    pos = (d["y"] == 1).sum()
    neg = (d["y"] == 0).sum()
    if pos == 0 or neg == 0:
        return 0.0
    d["tpr"] = (d["y"] == 1).cumsum() / pos
    d["fpr"] = (d["y"] == 0).cumsum() / neg
    return float((d["tpr"] - d["fpr"]).max())

def fit_eval(df, target, cat_cols, num_cols, thr=None):
    X = df.drop(columns=[target])
    y = df[target].astype(int)
    if "ID" in X.columns:
        X = X.drop(columns=["ID"])
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)
    pipe = build_pipeline(cat_cols, num_cols)
    pipe.fit(X_train, y_train)
    prob = pipe.predict_proba(X_test)[:, 1]
    fpr, tpr, thr_arr = roc_curve(y_test, prob)
    if thr is None:
        grid = np.linspace(0.01, 0.99, 99)
        f1_vals = []
        for t in grid:
            p = (prob >= t).astype(int)
            f1_vals.append(f1_score(y_test, p, pos_label=1))
        thr = float(grid[int(np.argmax(f1_vals))])
    pred = (prob >= thr).astype(int)
    auc = float(roc_auc_score(y_test, prob))
    cm = confusion_matrix(y_test, pred)
    metrics = {
        "threshold": thr,
        "precision_1": float(precision_score(y_test, pred, pos_label=1, zero_division=0)),
        "recall_1": float(recall_score(y_test, pred, pos_label=1)),
        "f1_1": float(f1_score(y_test, pred, pos_label=1)),
        "accuracy": float(accuracy_score(y_test, pred)),
        "auc": auc,
        "ks": ks_stat(y_test, prob),
        "confusion_matrix": cm.tolist(),
    }
    report = classification_report(y_test, pred, digits=3)
    return pipe, metrics, report, (fpr, tpr)

model, metrics, report, roc_pts = fit_eval(df, TARGET, cats, nums)
print(report)
print(metrics)

plt.figure(figsize=(8, 6))
sns.heatmap(metrics["confusion_matrix"], annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

plt.figure(figsize=(8, 6))
plt.plot(roc_pts[0], roc_pts[1], label=f'AUC = {metrics["auc"]:.2f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()

plt.figure(figsize=(8, 6))
plt.plot(roc_pts[0], roc_pts[1], label='Cumulative True Positives')
plt.plot(roc_pts[0], roc_pts[0], 'k--', label='Random Model')
plt.plot(roc_pts[0], roc_pts[1] - roc_pts[0], label='Difference (TPR - FPR)')
ks_value = metrics['ks']
ks_threshold = roc_pts[0][np.argmax(roc_pts[1] - roc_pts[0])]
plt.axvline(x=ks_threshold, color='red', linestyle='--', label=f'KS = {ks_value:.2f} at threshold {ks_threshold:.2f}')
plt.xlabel('Threshold')
plt.ylabel('Cumulative Probability')
plt.title('Kolmogorov-Smirnov (KS) Plot')
plt.legend(loc='lower right')
plt.show()

# ========================
# WoE & Scorecard con optbinning
# (celda lista para Google Colab)
# ========================

# 1) Instalación SOLO si falta (modo quiet)
try:
    import optbinning  # noqa: F401
except Exception:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "optbinning"])  # quiet
    import optbinning  # noqa: F401

# 2) Imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import display
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from optbinning import BinningProcess, Scorecard
from optbinning.scorecard import plot_auc_roc, plot_ks, plot_cap
import warnings # Import warnings

# 3) Tomamos objetos del notebook (df, cats, nums, TARGET) o los construimos si faltan
if "df" not in globals():
    # Se asume que en el notebook existe preprocess(), CSV_PATH, TARGET_RAW, TARGET
    # Add logic to load and preprocess data if df is not available
    try:
        df = preprocess(CSV_PATH, TARGET_RAW, TARGET)
        cats, nums = infer_types(df, target=TARGET)
    except Exception as e:
        print(f"Error during data loading and preprocessing: {e}")
        # Exit or handle the error appropriately if df is essential

if "cats" not in globals() or "nums" not in globals():
    # Assuming df is available at this point, infer types
    if 'df' in globals():
        cats, nums = infer_types(df, target=TARGET)
    else:
        print("Error: Dataframe 'df' not available to infer types.")
        # Exit or handle error if cats and nums are essential

# 4) Preparación de datos
TARGET_COL = TARGET  # mantiene consistencia con el resto del notebook
feature_cols = [c for c in df.columns if c != TARGET_COL]
X = df[feature_cols]
y = df[TARGET_COL].astype(int).values
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.30, stratify=y, random_state=42
)

# 5) BinningProcess (WoE por defecto) + selección simple por IV/calidad
# Define selection criteria for BinningProcess
selection_criteria = {
    "iv": {"min": 0.02, "max": 1.0},
    "quality_score": {"min": 0.01}
}

# Suppress FutureWarnings from optbinning/sklearn
with warnings.catch_warnings():
    warnings.simplefilter(action='ignore', category=FutureWarning)
    bp = BinningProcess(
        variable_names=feature_cols,
        categorical_variables=cats,        # trata estas como categóricas
        special_codes=[np.nan],            # NaN como código especial
        selection_criteria=selection_criteria,
    )

    bp.fit(X_train, y_train)
    bp_summary = bp.summary()

# 6) Transformación a WoE
#    Solo variables seleccionadas por el binning process
selected_vars = bp.get_support(names=True)
X_train_woe = pd.DataFrame(bp.transform(X_train, metric="woe"), columns=selected_vars)
X_test_woe  = pd.DataFrame(bp.transform(X_test,  metric="woe"), columns=selected_vars)

# 7) Scorecard (Logistic Regression sobre WoE) + escala 300-850
estimator = LogisticRegression(max_iter=1000, solver="lbfgs")
scorecard = Scorecard(
    binning_process=bp,
    estimator=estimator,
    scaling_method="min_max",                        # escala lineal
    scaling_method_params={"min": 300, "max": 850} # rango típico de score
)

scorecard.fit(X_train, y_train)
sc_table_summary = scorecard.table(style="summary")  # tabla corta (Variable, Bin, Points)
sc_table_detail  = scorecard.table(style="detailed") # tabla con métricas (IV, WoE, etc.)

"""# RESULTADOS


---


"""

y_prob = scorecard.predict_proba(X_test)[:, 1]
plt.figure(figsize=(6, 4)); plot_auc_roc(y_test, y_prob); plt.tight_layout(); plt.show()
plt.figure(figsize=(6, 4)); plot_cap(y_test, y_prob);     plt.tight_layout(); plt.show()
plt.figure(figsize=(6, 4)); plot_ks(y_test, y_prob);      plt.tight_layout(); plt.show()

scores_test = scorecard.score(X_test)
plt.figure(figsize=(8, 4))
plt.hist(scores_test[y_test == 0], bins=30, alpha=0.5, label="No-event (0)")
plt.hist(scores_test[y_test == 1], bins=30, alpha=0.5, label="Event (1)")
plt.title("Distribución de puntajes del scorecard (test)")
plt.xlabel("Puntaje"); plt.legend(); plt.tight_layout(); plt.show()

print("\nTabla de Scorecard (resumen)")
display(sc_table_summary)
